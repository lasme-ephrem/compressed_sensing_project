{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951c4d2c-fcc2-475f-ba4b-2c9c280aa24a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<u>Avertissement</u> : Ce rapport a été rédigé depuis un carnet jupyter lab. Il est également compatible sous jupyter notebook. Pour l'exécuter correctement, il faut lancer un jupyter notebook depuis le répertoire principal de ce projet : PROJET_CS_ESSOH_LASME_&_BERREBI_NATHANE pour s'assurer que tous les liens hypertextes du notebook sont respectés. Cela est nécessaire pour l'affichage des images, logos et les navigations entre les codes sources de la section implémentation SDP. Ce rapport n'a pas été conçu pour Google Colab et ne nous sommes pas sûrs de garantir la bonne exécution des liens hypertextes sur Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7552e4f-56d9-4692-9750-4e17aadd7da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> PROJET DE COMPRESSED SENSING </center>\n",
    "\n",
    "<center> <img src = \"img/logo_ensae.png\" height=\"300\" width=\"300\"/> </center>\n",
    "\n",
    "\n",
    "#### <center>  Réalisé par : </center>\n",
    "\n",
    "<center>BERREBI Nathane</center>\n",
    "<center>ESSOH Lasme Ephrem Dominique </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Elèves-Ingénieurs 3A DSSA (2021 - 2022)</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11926f77-c1ab-4a20-b537-b73f684f8842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Thème :  </center> \n",
    "\n",
    "# <center>Semidefinite relaxations for certifying robustness to adversarial examples</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf1842-3630-4909-afd4-ae80f70c12c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f684d46-19d9-485a-80ed-8084f2907e8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Revolution de l'IA par les réseaux de neurones artificiels\n",
    "\n",
    "- Application à la vision par odinateur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f75281-7399-47f6-b284-b3fe4f0bd8c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aversarials examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977e8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src = \"img/adversarial_example.png\" height=\"300\" width=\"600\"/> </center>\n",
    "Exemple (Jun Yan, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f83af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Solution immédiate :  \"course aux armements\"\n",
    "- Solution robuste : Relaxation convexe\n",
    "- Solution du papier (Raghunathan et al., 2018) : la relaxion SDP pour certifier la robustesse des réseaux de neurones multicouches avec fonction d'activation ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98617e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b> Nous proposons une implémentation d'un adversarial example à travers ce notebook : <a href = \"sources/generation_adv_example/generate_adv.html\">génération d'un exemple contraire.</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723c1d6-f426-4717-8181-5d0fbb08e764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Conséquences\n",
    "\n",
    "<center> <img src = \"img/adversarial_example_ac.png\" height=\"200\" width=\"400\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d9e8a-57f9-419e-9128-74ca5a281f5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Certification de la robustesse d'un réseau de neurones \n",
    "\n",
    "Nous présentons dans la suite un moyen de lutter contre les adversarials examples pour des réseaux de neurones multicouche avec fonction d'activation ReLU pour la classification. \n",
    "\n",
    "Ainsi, cette section présente le cadre mathématique de la notion de certification d'un réseau de neurones qui se ramène à un problème d'optimisation non convexe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84faacd6-09d5-4f9f-99b0-66d6b69c22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notations \n",
    "\n",
    "Soit $z \\in R^n$ un vecteur, nous désignons par $z_i$ sa $i-$ème composante.\n",
    "\n",
    "\n",
    "Soit $Z \\in R^{m \\times n}$ une matrine à $m$ lignes et $n$ colonnes, nous désignons par $Z_i$ sa $i-$ème ligne.\n",
    "\n",
    "Soit $f : R \\rightarrow R$ une fonction réelle et $z$ un vecteur de $z \\in R^n$, $f(z)$ désigne un vecteur de $z \\in R^n$ dont les composantes sont les $f(z_i) = (f(z))_i$.\n",
    "\n",
    "Pour $z,y ∈ R^n$, $z≽y$ désigne que $z_i ≥ y_i$ pour $i=1,2,...,n$. \n",
    "\n",
    "Nous utiliserons la notation $z_1 ⊙ z_2$ pour représenter le produit élément par élément des deux vecteurs.\n",
    "\n",
    "On définit $B_ε(\\bar{x}) = \\{x : ||x - \\bar{x}||\\infty ≤ε\\}$ pour désigner la boule $L_\\infty$ autour de $\\bar{x}$. \n",
    "\n",
    "Nous désignons le vecteur dont toutes les composantes sont zéros par $\\mathbb{0}$ et le vecteur dont toutes les composantes sont uns par $\\mathbb{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200382c5-cfd9-4f40-b8ed-843b72cf1682",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Réseaux neurones multicouches avec activation ReLU pour la classification\n",
    "\n",
    "On travail sur les réseaux de neurones muticouches avec fonction d'activation ReLU pour la classification. Un tel réseau de neurones avec $L$ couches cachées est une fonction $f$ définie comme suit : \n",
    "\n",
    " - soit $x^0 \\in R^d$ appelé \"input\",\n",
    "\n",
    " - soit $x^1, \\ldots, x^L$ appelés des \"vecteurs d'activations\" sur les couches cachées,\n",
    "\n",
    " - supposons que $m_i \\in N$ est le nombre d'unités d'une couche $i$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f182d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "alors \n",
    "\n",
    "\n",
    " - $x^i$ est lié à $x^{i-1}$ par la relation $x^i = max(W^{i-1}x^{i-1}, 0) = ReLU (W^{i-1}x^{i-1})$, où $ W^{i-1} \\in R^{m_i \\times m_{i-1}}$ sont les poids du réseau (on a omis les terme de biais),\n",
    "\n",
    "<br>\n",
    " \n",
    " - et $f(x^0)$ est un vecteur de $R^k$, obtenu à travers les transformations successives de $x^0$ à $x^L$ suivant la relation de récurrence ci-dessus, tel que $f(x^0)_j = c_j^Tx^L$ represente le score de la classe $j$. l'étiquette $y$ à assigner à un input $x^0$ est donc la classe avec le plus gros score : $y = argmax_{j = 1, \\ldots, k}f(x^0)_j$\n",
    " \n",
    " \n",
    " \n",
    " <center> <img src = \"img/ff_nn.png\" height=\"200\" width=\"400\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a5ec5-fb92-41c0-856b-b96f8a9a4d55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modèle de l'attaquant et certification de robustesse\n",
    "\n",
    "Etant donnée un réseau de neuronnes multicouches avec activations ReLU, \n",
    "\n",
    "- Hypothèse de présence d'un attaqueur $A : R^d \\rightarrow R^d$. $A$ prend un input correct $\\bar{x} \\in R^d$ et renvoie une version pertubée de cet input $A(\\bar{x})$ qui est passée au réseau de neurones au lieu du vrai input. \n",
    "\n",
    "- On s'intéresse aux attaquants bornés suivant la norme $L_{\\infty}$ : $A(\\bar{x}) \\in B_{\\varepsilon}(\\bar{x})$ pour un certain $\\varepsilon > 0$.\n",
    "\n",
    "- L'attaquant $A$ est correcte si pour une paire d'observation $(\\bar{x}, \\bar{y})$, la prédiction fournie par le réseau en $A(\\bar{x})$ est différente de $\\bar{y}$:  $f(A(\\bar{x})) \\ne \\bar{y}$ \n",
    "\n",
    "- Le score rapporté par le réseau en $A(\\bar{x})$ pour n'importe quelle classe $y \\ne \\bar{y}$ excède le score rapporté par le réseau en $A(\\bar{x})$ pour la classe $\\bar{y}$, ce que l'on peut écrire $f(A(\\bar{x}))_y > f(A(\\bar{x}))_{\\bar{y}}$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7afb7b-110e-4a5c-99b0-05f8cdf8d7c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En supposant que l'attaquant connait le réseau, un bonne manière de limiter son impact est de s'intéresser à la pire penalité possible que l'attaqueur peut infliger au réseau quand il fournit une classe incorrecte $y$ au lieu de $\\bar{y}$. Cette pénalité est en fait :\n",
    "\n",
    "$$l^*_y(\\bar{x}, \\bar{y})  = max_{A(x) \\in B_{\\varepsilon}(\\bar{x})} ( f(A(x))_y - f(A(x))_{\\bar{y}} )$$\n",
    "\n",
    "\n",
    "On dira alors qu'un réseau de neurones est cértifié robuste sur l'exemple $(\\bar{x}, \\bar{y})$ si $l^*_y(\\bar{x}, \\bar{y})<0$ pour tout $y \\ne \\bar{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5ac72-f480-4170-9be3-504f893e8c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formulation du problème en terme d'optimisation\n",
    "\n",
    "* Soit $y$ une classe fixée, une paire d'exemple $(\\bar{x}, \\bar{y})$. La pire pénalité $l^*_y(\\bar{x}, \\bar{y})$ d'un réseau de neurones $f$ avec des poids $W$ peut s'exprimer comme un problème d'optimisation. \n",
    "\n",
    "* Variable de décision : $A(x)$ que nous noterons sans perte de généralité $x^0$. \n",
    "\n",
    "* Fonction objective : $f(x^0)_y - f(x^0)_{\\bar{y}} = (c_y - c_{\\bar{y}})^Tx^L$, avec où $x^L$ est l'activation de la couche finale.\n",
    "\n",
    "En imposant les contraintes adéquates dictées par le réseau de neurones et les constraintes de l'attaqueur A, $l^*_y(\\bar{x}, \\bar{y})$ est donné par le programme suivant :\n",
    "\n",
    "$$ l^*_y(\\bar{x}, \\bar{y}) = max_{x^0, \\ldots, x^L} (c_y - c_{\\bar{y}})^Tx^L $$\n",
    "\n",
    "<center>Sous contrainte : $ReLU (W^{i-1}x^{i-1})$ pour $i = 1 \\ldots L$ (contraintes du réseau)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: x^i = ||x^0_j - \\bar{x}_j||\\infty ≤ε$ pour $j = 1 \\ldots d$ (contraintes du modèle d'attaque)</center>\n",
    "\n",
    "* D'un point de vu informatique, calculer $l^*$ est NP-Hard (optimisation non-convexe)\n",
    "* Idée de solution : Relaxation convexe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa88c1-cdab-4fc1-9eff-656db540f0a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relaxation du problème d'optimisation\n",
    "\n",
    "Nous étudions dans ici une relaxation convexe qui permet de calculer une borne supérieure de $l^*_y$ : $L_y(\\bar{x}, \\bar{y}) \\ge l^*_y(\\bar{x}, \\bar{y})$. De ce fait, comme discuté précédemment lorsque $L_y(\\bar{x}, \\bar{y}) < 0$, nous avons une certification de la robustesse du réseau pour l'input $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc88ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### Transformation du problème en un problème QCQP\n",
    "\n",
    "* La source de la non-convexité du programme d'optimisation défini dans la section précédente vient des contraintes ReLU du réseau. Contrainte ReLU : $z = max(x,0)$. \n",
    "\n",
    "* Peut s'exprimer de manière équivalente sous les trois contraintes suivantes : \n",
    "  - $(i) \\:\\:\\: z \\ge x$ \n",
    "  - $(ii) \\:\\:\\: z \\ge 0$\n",
    "  - $(iii) \\:\\:\\: z(z-x) = 0$.\n",
    "  \n",
    "\n",
    "* Cette reformulation de la contrainte ReLU nous permet de remplacer des contraintes non linéaires du problème d'optimisation de base par des contraintes linéaires et quadratiques :  QCQP.\n",
    "\n",
    "* Programmes QCQP en général NP-Hard mais solution par relaxation convexe SDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c16eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Rélaxation d'un réseau de neurones multicouches ReLU\n",
    "\n",
    "Nous montrons d'abord comment ce QCQP peut être relaxé en un programme SDP pour les réseaux à une couche cachée. La relaxation pour les couches multiples est une extension simple et est présentée à la fin de cette section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02604158",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "##### <u>Cas d'un réseau de neurones à une seule couche ReLU</u> :\n",
    "\n",
    "* Considérons un réseau de neurones avec un seule couche cachée de $m$ unités. \n",
    "\n",
    "* Soit un input $x \\in R^d$ du réseau. L'activation de la couche cachée sera notée $z \\in R^m$. Elle est reliée à $x$ par la relation $z = ReLU(Wx)$ où $W \\in R^{m \\times d}$ sont les poids du réseau.\n",
    "\n",
    "* Supposons que nous notre input est borné i.e. nous avons $l, u \\in R^d$ tels que $l_j \\le x_j \\le u_j$. \n",
    "\n",
    "    -Par exemple, dans le cas de notre modèle d'attaqueur $L_\\infty$, nous avons $l = \\bar{x} - \\varepsilon \\mathbb{1}$ et $l = \\bar{x} =u + \\varepsilon \\mathbb{1}$ où $\\bar{x}$ est un input correcte et non pertubé.\n",
    "    \n",
    "* Ce qui est équivalent à une contrainte quadratique $(x_j -l_j)(x_j -u_j)≤0$, que nous réécrivons comme suit $x^2_j ≤(l_j +u_j)x_j -l_ju_j$. \n",
    "    \n",
    "* Nous nous intéressons ici à la maximisation de l'objectif : $f(x) = c^Tz$ où $c \\in R^m$, $c = c_y - c_{\\bar{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299935a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Nous avons alors le QCQP suivant :\n",
    "\n",
    "\n",
    "$$l^*_y(\\bar{x}, \\bar{y}) = f_{QCQP} =  max_{x,z} c^Tx^L $$\n",
    "\n",
    "<center>Sous contrainte : $z \\ge 0, z \\ge Wx, z^2 = z ⊙ (Wx)$ (contraintes du réseau)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  x^2 ≤ (l+u)⊙x - l ⊙ u $  (contraintes du modèle d'attaque)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29d636-6c60-4d98-8d11-454bd50c151a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* On relaxe le QCQP non convexe précédent par une SDP convexe. \n",
    "\n",
    "* L'idée est d'introduire un nouvel ensemble de variables représentant tous les monômes linéaires et quadratiques en x et z ; les contraintes du programme ci-dessus peuvent alors être écrites comme des fonctions linéaires de ces nouvelles variables.\n",
    "\n",
    "* Par exemple, en posant $P = vv^T$ où $ v =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\[0.5mm]\n",
    "x\\\\[0.5mm]\n",
    "z\\\\\n",
    "\\end{pmatrix}\n",
    "$. Ainsi $P =  \\begin{pmatrix} \n",
    "1 & x & z \\\\ \n",
    "x & x^2 & xz \\\\ \n",
    "z & xz & z^2\n",
    "\\end{pmatrix}$ =  $\\begin{pmatrix} \n",
    "P[1] & P[x^T] & P[z^T] \\\\ \n",
    "P[x] & P[xx^T] & P[xz^T] \\\\ \n",
    "P[z] & P[zx^T] & P[zz^T]\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fc4a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ainsi, la relaxation convexe du programme principal peut sécrire comme suit :\n",
    "\n",
    "$$f_{SDP} =  max_{P} c^TP[z] $$\n",
    "\n",
    "<center>Sous contrainte : $P[x] \\ge 0, P[z] \\ge WP[x], diag(P[zz^T]) = diag(WP[xz^T])$ (contraintes ReLU)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  diag(P[xx^T]) \\le (l+u)⊙P[x] - l ⊙ u$  (contraintes du modèle d'attaque)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  P[1] = 1, P≽0$  (contraintes du matriciels)</center>\n",
    "\n",
    "\n",
    "* Remarquons que nous considérons l'ensemble des matrices $P$ telles que $P ≽ 0$. Cet ensemble est convexe et est un sur-ensemble de l'ensemble des contraintes original non-convexe.\n",
    "\n",
    "- Le programme ci-dessus est un programme SDP fournissant une relaxation du QCQP avec $f_{SDP} ≥ f_{QCQP}$ , fournissant une borne supérieure sur $l^*_y(\\bar{x}, \\bar{y})$ qui pourrait servir de certificat de robustesse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670d9fb-2ddd-4dd3-86e3-32f98bd125c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <u> Cas d'un réseau de neurones multicouches ReLU </u> :\n",
    "\n",
    "* La relaxation SDP pour évaluer la robustesse des réseaux multicouches est une généralisation directe de la relaxation présentée pour une couche cachée. \n",
    "\n",
    "* Les interactions entre $x_{i-1}$ et $x_i$ selon la contrainte ReLU  sont analogues à l'interaction entre la couche d'entrée $x$ et la couche cachée $z$ pour le cas à une couche. \n",
    "\n",
    "* Supposons que nous ayons des bornes $l^{i-1}$, $u^{i-1} \\in R^{m_{i-1}}$ sur les entrées des unités ReLU de la couche i telles que $l^{i-1} ≤x^{i-1} ≤u^{i-1}$. \n",
    "\n",
    "* En écrivant les contraintes pour chaque couche de manière itérative, on obtient le programme SDP suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470c650-c7fc-4d8f-808a-7058e56ecde4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$f^{SDP}_y(\\bar{x}, \\bar{y}) =  max_{P} (c_{y} - c_{\\bar{y}} )^T P[x^L] $$\n",
    "\n",
    "<center>Sous contrainte : pour  $ i =  1, \\ldots, L$ </center>\n",
    "\n",
    "<center>$P[x^i] \\ge 0, P[x^i] \\ge W^{i - 1}P[x^{i - 1}]$ </center>\n",
    "\n",
    "<center>$\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:diag(P[x^i(x^i)^T]) = diag(WP[x^{i - 1}(x^i)T])$ (contraintes ReLU)</center>\n",
    "\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: \\:\\:\\: diag(P[x^{i - 1}(x^{i - 1})^T]) \\le (l^{i - 1}+u^{i - 1})⊙P[x^{i - 1}] - l^{i - 1} ⊙ u^{i - 1}$  (contraintes du modèle d'attaque)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  P[1] = 1, P≽0$  (contraintes du matriciels)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f2808-983c-4f77-b443-1ae7316e1e57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expérimentation\n",
    "\n",
    "Dans cette section nous présentatons comment les auteurs du papier que nous étudions ont évalué leur méthode de relaxation SDP.\n",
    "\n",
    "Le réseau de neurones sur lequel ils travaillent est un réseau multicouches avec fonction d'activation ReLU destiné à classfier le célèbre de jeu de donnée MNIST (pour Modified ou Mixed National Institute of Standards and Technology, est une base de données de chiffres écrits à la main) pour une tâche de classification.\n",
    "\n",
    "Sur MNIST, les auteurs évaluent leur procédure de certification robuste par relaxation SDP sur trois architectures de réseau de neurones différentes et par rapport à deux autres procédures de certification distinctes : la relaxation LP et la certification par la méthode dite du gradient traités dans le cadre de papiers anciens à la publication sur laquelle nous travaillons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbee39-c3cb-4c69-8add-6e664e0ab455",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Les architectures de réseaux sont les suivantes : \n",
    "\n",
    "- L'arichtecture Grad-NN : c'est un réseau de neurones avec deux couches cachées de 500 unités chacunes et robuste à des adversarials examples suivant la procédure de certification du gradient. Ce réseau n'a pas été re-entrainé, ses poids ont été récupéré par les auteurs auprès de leur pairs qui ont inventé et entrainé ce réseau ;\n",
    "\n",
    "- L'arichtecture LP-NN : c'est un réseau de neurones avec deux couches cachées de 500 unités chacunes et robuste à des adversarials examples suivant la procédure de certification par la relaxtion LP. Ce réseau n'a, aussi, pas été re-entrainé, ses poids ont été récupéré par les auteurs auprès de leur pairs qui ont inventé et entrainé ce réseau ;\n",
    "\n",
    "- L'architecture PGD-NN : C'est un réseau de neurones avec quatre couches contenant 200, 100 et 50 unités entrâiné en utilisant des adversarials examples contre l'attaquant de déscente de gradient projété. Ce réseau a été entrainé par les auteurs de sorte à ce qu'il minimise une combinaison pondérée de la perte d'entropie croisée régulière et de la perte adversariale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66ae4e-0ad9-45cf-b1ec-1ebe1e12d39c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Les procédures de certifications : \n",
    "\n",
    "- SDP-cert. Il s'agit du certificat que les auteurs ont proposé : la certfication par relaxation SDP.\n",
    "\n",
    "- LP-cert. Il s'agit d'une méthode de certification basée sur la relaxation LP traitée dans la litterature précédente.\n",
    "\n",
    "- Grad-cert. Il s'agit d'une méthode certification basée sur la norme maximale du gradient des prédictions du réseau.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625294b4-af41-4c0e-9404-39798c0718a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Résultats : \n",
    "\n",
    "Le tableau ci-dessous présente les performances des trois différentes procédures de certification sur les trois réseaux. Pour chaque méthode de certification et chaque réseau, les auteurs évaluent les limites supérieures de $l^*$ (pire pénalité infligé par un attaqueur au réseau) sur 1000 mêmes images, choisi aléatoirement, dans le jeu de données de test de MNIST. A partir des limités supérieurs, les auteurs indiquent la fraction d'images qui n'ont pas été certifiés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05db608-172f-4a42-98af-27474a0cb502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center> <img src = \"img/results.png\" height=\"1000\" width=\"600\"/> </center>\n",
    " \n",
    " A partir de ce tableau, nous observons que SDP-cert est systématiquement plus performant que LP-cert et Grad-cert pour les trois réseaux. Grad-cert et LP-cert fournissent des certificats \"vides\" (erreur > 90%) sur des réseaux qui ne sont pas entraînés à minimiser ces certificats. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0105d0-bdb4-4b4f-b744-8ae92144a4a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Implémentation SDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787c375",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En ce qui concerne l'implémentation du problème traité dans ce papier, nous nous sommes reférés des à sources relatifs au papier que nous avons trouvé sur internet. Nous avons globalement mis à jour les lignes de codes non fonctionnelles notamment les lignes de codes fonctionnent avec  tensorflow sous sa version 1 alors que nous sommes aujourd'hui à la version 2 et plusieurs changement ont été apportés dans l'API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042fa24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implémentation des auteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175b9cb-5ea9-4b97-ab27-ee4f0b9b9ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour la réplication des résultats des auteurs il faut disposer de l'ensemble des codes sources, en Python et en Matlab, qui se trouvent dans le répertoire : <a href = \"http://localhost:8888/lab/tree/sources/\"> Fichiers sources</a> (/sources). \n",
    "    \n",
    "Ce répertoire est organisé de la manière suivante : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fccbb-c6f8-4290-a0a4-a154f1cb5153",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- *code* : contient tous les sources pour l'entrainement ou le transfert du réseau avec les différentes certifications. En particulier, il contient le fichier matlab - <a href = \"http://localhost:8888/lab/tree/sources/code/matlab_sdp.m/\">  matlab_sdp.m </a> (/sources/codes/matlab_sdp.m) - qui implémente la relaxation SDP sur des réseaux multicouches. C'est ce fichier qui communique avec Python via le module de matlab en Python : *matlab.engines* pour caculer les limites supérieures de certification. Notons que matlab.engines nécessite l'installation de matlab sur la machine qui l'exécute ; \n",
    "\n",
    "\n",
    "    \n",
    "- *mnist_permuted_data* : contient les 1000 images annotées pour lesquelles une certification de robustesse est appliquée ;\n",
    "\n",
    "\n",
    "- *models* : contient les fichiers de sauvegarde tensorflow relatifs aux trois différentes architectures de réseaux de neurones mentionnées précédemment. \n",
    "\n",
    "\n",
    "L'ensemble des répertoires sont liés et au fichiers source principale :  <a href = \"http://localhost:8888/lab/tree/sources/code/certify.py\"> certify.py</a> (/sources/codes/certify.py). C'est ce fichier qu'il faut exécuter depuis un terminal Python dans un environnement de programmation où sont installés les modules suivants : *numpy, matplotlib, scikit-learn, tensorflow et matlab.engines*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda2f00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notre implémentation (dérivée de celle des auteurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c268cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notre implémentation est dérivée de celle des auteurs, elle utilise les principaux codes sources de leur projet afin de proposer une éxécution plus aisée de la relaxation SDP sur un réseau de neurones multicouches sous son ordinateur personnel, tout minisant le temps passé pour déboger le code originel des auteurs. Notre implémentation est inspirée de : https://github.com/soc-ucsd/SDPfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bec61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour utiliser cette implémentation, il faut juste créer un environnement python utilisant le fichier <a href = \"http://localhost:8888/lab/tree/verification/requirements.txt\"> requirements</a>.\n",
    "\n",
    "Il aussi possible de créer son propre environnement puis installer les modules suivants (ce qui revient au même que la méthode précédente) : numpy sklearn tensorflow matlab.engines Mosek\n",
    "\n",
    "Notons que l'installation de matlab.engines et Mosek implique par ailleurs d'installer le logiciel <a href = \" https://fr.mathworks.com/products/matlab.html\"> Matlab</a> et sa toolbox additive  <a href = \"https://www.mosek.com/\"> Mosek</a>  pour la résolution de programme d'optimisation convexes sophistiqués tels que les SDP.\n",
    "\n",
    "(Faire la démonstration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cbda5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un fois tous les prérequis d'installation réussi, on peut lancer le test de robustesse soit sur le réseau des auteurs (pour la réplication des résultats) soit sur un réseau personnel (nous avons choisi le jeu de données iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab639fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sur le réseau des auteurs\n",
    "!cd verification\n",
    "!python index.py --dataset MNIST --nnfile model/raghunathan18_pgdnn.pkl --eps 0.1 --dims \"[784, 200, 100, 50, 10]\" --method \"sdpnet\" --num 30 --input_bounds \"(0., 1.)\" --output mnist_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c7837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sur un réseau personnel\n",
    "!python train.py --dataset iris --dims \"[4, 5, 10, 20, 30, 40, 3]\" #création de l'architecture du réseau et entrainement\n",
    "\n",
    "#certification de la robustesse du réseau\n",
    "!python index.py --dataset iris --nnfile params.pkl --eps 0.075 --dims \"[4,5,10,20,30,40,3]\" --num 30 --input_bounds \"(0., 10.)\" --method \"sdpnet\"   --output iris_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be8780",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A la fin du processus de certification, un fichier de sortie (log) est imprimée afin de constater le taux d'exemples non certifiés. A noter que la procédure de certification peut prendre du temps de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367caaa-d939-4f8b-a24e-b1a86416f5e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "En conclusion, dans leur article, Raghunathan et al. (2018) proposent une nouvelle relaxation : la relaxion SDP pour certifier la robustesse qui s'applique aux réseaux ReLU pour la classification. Ils montrent que la relaxation qu’ils proposent est plus meilleure que les relaxations précédentes à leur papier et qu'elle produit des garanties de robustesse significatives sur trois réseaux étrangers différents dont les objectifs de formation sont agnostiques par rapport à la relaxation qu’ils proposent."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
