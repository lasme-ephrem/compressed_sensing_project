{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586ad85e-c2a0-4dc7-9775-4bb9093758cd",
   "metadata": {},
   "source": [
    "# <center> PROJET DE COMPRESSED SENSING </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7552e4f-56d9-4692-9750-4e17aadd7da4",
   "metadata": {},
   "source": [
    "<center> <img src = \"logo_ensae.png\" height=\"300\" width=\"300\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523f286-097c-4db7-9e8c-d9beb211ad88",
   "metadata": {},
   "source": [
    "#### <center>  Réalisé par : </center>\n",
    "\n",
    "<center>BEREBI Nathane</center>\n",
    "<center>ESSOH Lasme Ephrem Dominique </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Elèves-Ingénieurs 3A DSSA</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11926f77-c1ab-4a20-b537-b73f684f8842",
   "metadata": {},
   "source": [
    "#### <center> Thème :  </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17020c-1f4e-4a0b-85fb-942912de0535",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center>Semidefinite relaxations for certifying robustness to adversarial examples</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683aa857-561b-4eeb-9432-9191a3a71c40",
   "metadata": {},
   "source": [
    "Ce travail s'inscrit dans le cadre du cours de Compressed Sensing. Il porte sur le Papier de Aditi Raghunathan, Jacob Steinhardt et Percy Liang présenté au NeurIPS en 2018. Dans ce papier, il s'agit essentiellement d'apporter une solution à l'échec de prédiction des réseaux de neurones en face \"d'adversarial examples\", la solution proposée étant basée sur la relaxation convexe SDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf1842-3630-4909-afd4-ae80f70c12c0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f684d46-19d9-485a-80ed-8084f2907e8f",
   "metadata": {},
   "source": [
    "Au cours de cette dernière décénie, pas un seul jour ne passe sans que les prouesses des réseaux de neurones artificiels soient mises en évidences que ce soit à travers les médias qu'à travers la prolifération de publications scientifiques sur le sujet. Ces réseaux sont à la base de la révolution de l'Intelligence Artificielle surtout dans le domaine de la vision par ordinateur. Cependant,  malgré leurs performances impressionnantes dans diverses tâches relatifs à la vision, les réseaux de neurones peinent, de manière catastrophique, à prédire correctement l'étiquette associée à une image si les pixels de celle-ci sont pertubés par un bruit de corruption imperceptible par ces réseaux de neurones. Ainsi, une image d'un Panda dont les pixels ont été pertubées pourrait être perçue comme étant celle d'un Gibbon alors qu'un être humaine perçevrait clairement un Panda : c'est la problématique d'un \"adversarial example\", c'est-à-dire celle d'exemple d’objet capable de tromper et déjouer un algorithme d’un réseau de neurones en lui faisant croire qu’il doivent être classifié en tant que tel objet alors que ce n’est pas le cas.\n",
    "<br>\n",
    "\n",
    "Pour résoudre ce problème des exemples contradictoires, la façon la plus simple et naturelle de procéder est de construire des réseaux entrainés sur des adversariales training en générant un certain nombre d’adversarial examples contre notre réseau de neurones puis à entraîner notre réseau de neurones sur ces données générées. Cette façon de procédé à conduit à \"une course aux armements\" entre les défenseurs qui tentent de former des réseaux robustes par les adversariables learning et les attaquants qui essaient de construire des exemples adverses nouveaux biaisant ces réseaux robustes. Ainsi, au risque de proposer un moyen efficace de se défendre contre les exemples contradictoires, cette courses aux armements pour être sans fin.\n",
    "\n",
    "<br>\n",
    "\n",
    "L'une des stratégie pour mettre fin à cette course aux armements est de développer des \"défenses certifiées\", c'est-à-dire des défenses dont la robustesse contre tous les attaquants d'une certaine famille est prouvée. Ces défenses certifiées sont basées sur des relaxations convexes qui construisent une limite supérieure sur la perte la plus défavorable pour tous les attaquants de la famille. \n",
    "\n",
    "<br>\n",
    "Dans leur article, Raghunathan et al. proposent une nouvelle relaxation : la relaxion SDP pour certifier la robustesse des réseaux de neurones multicouches avec fonction d'activation ReLU.\n",
    "\n",
    "<br> Notre travail, basé sur l'article de Raghunathan et al.(2018), décrit comment appliquer une relaxation dans le cadre du jeu de données MNIST et en présence d'adversarials examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f75281-7399-47f6-b284-b3fe4f0bd8c2",
   "metadata": {},
   "source": [
    "# Aversarials examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98667714-fbc8-4661-8788-6c22a658087d",
   "metadata": {},
   "source": [
    "<b>Définition : </b> Un “adversarial example” ou exemple contradictoire est un exemple d’objet capable de tromper et déjouer un algorithme d’un réseau de neurones en lui faisant croire qu’il doit être classifié en tant que tel objet alors que ce n’est pas le cas. \n",
    "\n",
    "<br> \n",
    "\n",
    "<b> Caractérisation </b> : Un “avdersarial example” est un ensemble de données correctement initialisées auxquelles on aurait ajouté une perturbation imperceptible par le réseau de neurones afin d’entraîner une mauvaise classification.\n",
    "\n",
    "<br> \n",
    "\n",
    "<b> Exemple (Jun Yan, 2019) : </b> Prenons l'exemple de la classification d’images sur les pandas qu’un réseau de neurones reconnaît correctement comme un panda avec un taux de confiance de 57,7 %. Si on ajoute un peu de perturbation soigneusement construite,  le même réseau de neurones classe maintenant l’image comme étant un gibbon avec une confiance de 99,3 % ! Il s’agit clairement d’une illusion d’optique, mais seulement pour le réseau de neurones. Nous, humains, pouvons clairement affirmer que ces deux images correspondent bien à des pandas. En fait, nous ne pouvons même pas percevoir qu’un peu de perturbation a été ajoutée à l’image originale de gauche pour construire l’adversarial example à droite !\n",
    "\n",
    "<center> <img src = \"adversarial_example.png\" height=\"300\" width=\"600\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723c1d6-f426-4717-8181-5d0fbb08e764",
   "metadata": {},
   "source": [
    "Les adversarials examples posent de sérieux problèmes qui pourraient se transposer dans notre quotidienne réelle. Par exemple, en ce qui concerne la conduite de véhicule autonome, une mauvaisse détection des panneaux de signalisation peut entraîner des conséquences dangereuses. En effet, Eykholt et al (2018) ont présenté comment des réseaux de neurones embarqués dans véhicules autonomes souffraient à détecter un panneau sur lequel on a collé des stickers ou réalisé des graffitis. Le problème a été illustré par les auteurs avec un panneau stop (image à gauche dont la représentation pertubée à droite) qui a été classifié comme étant une limitation de vitesse à 45 km/h !\n",
    "\n",
    "<center> <img src = \"adversarial_example_ac.png\" height=\"200\" width=\"400\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d9e8a-57f9-419e-9128-74ca5a281f5a",
   "metadata": {},
   "source": [
    "# Certification de la robustesse d'un réseau de neurones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299423b8-5fe5-4182-b93a-aee5346b528d",
   "metadata": {},
   "source": [
    "Nous présentons dans la suite un moyen de lutter contre les adversarials examples pour des réseaux de neurones multicouche avec fonction d'activation ReLU pour la classification. Ainsi, cette section présente le cadre mathématique de la notion de certification d'un réseau de neurones qui se ramène à un problème d'optimisation non convexe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84faacd6-09d5-4f9f-99b0-66d6b69c22aa",
   "metadata": {},
   "source": [
    "### Notations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929821bd-9966-40fb-a21c-8c3e44a09a75",
   "metadata": {},
   "source": [
    "Soit $z \\in R^n$ un vecteur, nous désignons par $z_i$ sa $i-$ème composante.\n",
    "\n",
    "\n",
    "Soit $Z \\in R^{m \\times n}$ une matrine à $m$ lignes et $n$ colonnes, nous désignons par $Z_i$ sa $i-$ème ligne.\n",
    "\n",
    "Soit $f : R \\rightarrow R$ une fonction réelle et $z$ un vecteur de $z \\in R^n$, $f(z)$ désigne un vecteur de $z \\in R^n$ dont les composantes sont les $f(z_i) = (f(z))_i$.\n",
    "\n",
    "Pour $z,y ∈ R^n$, $z≽y$ désigne que $z_i ≥ yi$ pour $i=1,2,...,n$. \n",
    "\n",
    "Nous utiliserons la notation $z_1 ⊙ z_2$ pour représenter le produit élément par élément des deux vecteurs.\n",
    "\n",
    "On définit $B_ε(\\bar{x}) = \\{x : ||x - \\bar{x}||\\infty ≤ε\\}$ pour désigner la boule $L_\\infty$ autour de $\\bar{x}$. \n",
    "\n",
    "Nous désignons le vecteur dont toutes les composantes sont zéros par $\\mathbb{0}$ et le vecteur dont toutes les composantes sont uns par $\\mathbb{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200382c5-cfd9-4f40-b8ed-843b72cf1682",
   "metadata": {},
   "source": [
    "### Réseaux neurones multicouches avec activation ReLU pour la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c236-a3ee-4c03-b341-4a0c89b20402",
   "metadata": {},
   "source": [
    "Ce travail se concentre sur les réseaux de neurones muticouches avec fonction d'activation ReLU pour la classification. Un tel réseau de neurones avec $L$ couches cachées est une fonction $f$ définie comme suit : \n",
    "\n",
    " - soit $x^0 \\in R^d$ appelé \"input\",\n",
    "\n",
    " - soit $x^1, \\ldots, x^L$ appelés des \"vecteurs d'activations\" sur les couches cachées,\n",
    "\n",
    " - supposons que $m_i \\in N$ est le nombre d'unités d'une couche $i$,\n",
    "\n",
    "alors \n",
    "\n",
    "\n",
    " - $x^i$ est lié à $x^{i-1}$ par la relation $x^i = max(W^{i-1}x^{i-1}, 0) = ReLU (W^{i-1}x^{i-1})$, où $ W^{i-1} \\in R^{m_i \\times m_{i-1}}$ sont les poids du réseau (on a omis les terme de biais),\n",
    "\n",
    "<br>\n",
    " \n",
    " - et $f(x^0)$ est un vecteur de $R^k$, obtenu à travers les transformations successives de $x^0$ à $x^L$ suivant la relation de récurrence ci-dessus, tel que $f(x^0)_j = c_j^Tx^L$ represente le score de la classe $j$. l'étiquette $y$ à assigner à un input $x^0$ est donc la classe avec le plus gros score : $y = argmax_{j = 1, \\ldots, k}f(x^0)_j$\n",
    " \n",
    " <center> <img src = \"ff_nn.png\" height=\"200\" width=\"400\"/> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a5ec5-fb92-41c0-856b-b96f8a9a4d55",
   "metadata": {},
   "source": [
    "### Modèle de l'attaquant et certification de robustesse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7afb7b-110e-4a5c-99b0-05f8cdf8d7c9",
   "metadata": {},
   "source": [
    "Etant donnée un réseau de neuronnes multicouche avec activations ReLU, on suppose que l'univers des inputs est en présence d'un attaqueur $A$ qui prend un input correct $\\bar{x} \\in R^d$ et renvoie une version pertubée de cet input $A(\\bar{x})$ qui est passée au réseau de neurones au lieu du vrai input. De ce fait un attaqueur est une application $A : R^d \\rightarrow R^d$. Dans ce travail, nous nous intéressons aux attaquants qui sont bornés suivant la norme $L_{\\infty}$ : $A(\\bar{x}) \\in B_{\\varepsilon}(\\bar{x})$ pour un certain $\\varepsilon > 0$.\n",
    "\n",
    "L'attaquant $A$ est correcte si pour une paire d'observation $(\\bar{x}, \\bar{y})$, la prédiction fournie par le réseau en $A(\\bar{x})$ est différente de $\\bar{y}$ : c'est-à-dire $f(A(\\bar{x})) \\ne \\bar{y}$ ou de manière équivalente le score rapporté par le réseau en $A(\\bar{x})$ pour n'importe quelle classe $y \\ne \\bar{y}$ excède le score rapporté par le réseau en $A(\\bar{x})$ pour la classe $\\bar{y}$, ce que l'on peut écrire $f(A(\\bar{x}))_y > f(A(\\bar{x}))_{\\bar{y}}$ .\n",
    "\n",
    "En supposant que l'attaquant connait le réseau, un bonne manière de limiter son impact est de s'intéresser à la pire penalité possible que l'attaqueur que peut infliger au réseau quand il fournit une classe incorrecte $y$ au lieu de $\\bar{y}$. Cette pénalité est en fait :\n",
    "\n",
    "$$l^*_y(\\bar{x}, \\bar{y})  = max_{A(x) \\in B_{\\varepsilon}(\\bar{x})} ( f(A(x))_y - f(A(x))_{\\bar{y}} )$$\n",
    "\n",
    "\n",
    "On dira alors qu'un réseau de neurones est cértifié robuste sur l'exemple $(\\bar{x}, \\bar{y})$ si $l^*_y(\\bar{x}, \\bar{y})<0$ pour tout $y \\ne \\bar{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5ac72-f480-4170-9be3-504f893e8c11",
   "metadata": {},
   "source": [
    "### Formulation du problème en terme d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ff117-1038-414d-bde4-eccebe552e29",
   "metadata": {},
   "source": [
    "Pour une classe $y$ fixée et pour une paire d'exemple $(\\bar{x}, \\bar{y})$, la pire pénalité $l^*_y(\\bar{x}, \\bar{y})$ d'un réseau de neurones $f$ avec des poids $W$ peut s'exprimer comme un problème d'optimisation. La variable de décision est l'input $A(x)$ que nous noterons sans perte de généralité $x^0$. La fonction objectif est $f(x^0)_y - f(x^0)_{\\bar{y}} = (c_y - c_{\\bar{y}})^Tx^L$ qui est maximiser, avec où $x^L$ est l'activation de la couche finale i.e. la dernière transformation succéssive de $x^0$ suivant les couches cachées à travers le réseau. \n",
    "\n",
    "En imposant les contraintes adéquates dictées par le réseau de neurones et les constraintes de l'attaqueur A, $l^*_y(\\bar{x}, \\bar{y})$ est donné par le programme suivant :\n",
    "\n",
    "$$ l^*_y(\\bar{x}, \\bar{y}) =. max_{x^0, \\ldots, x^L} (c_y - c_{\\bar{y}})^Tx^L $$\n",
    "\n",
    "<center>Sous contrainte : $ReLU (W^{i-1}x^{i-1})$ pour $i = 1 \\ldots L$ (contraintes du réseau)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: x^i = ||x^0_j - \\bar{x}_j||\\infty ≤ε$ pour $j = 1 \\ldots d$ (contraintes du modèle d'attaque)</center>\n",
    "\n",
    "D'un point de vu informatique, calculer $l^*$ est difficile en général, c'est pourquoi nous proposons une relaxation de cet objectif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa88c1-cdab-4fc1-9eff-656db540f0a1",
   "metadata": {},
   "source": [
    "### Relaxation du problème d'optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29d636-6c60-4d98-8d11-454bd50c151a",
   "metadata": {},
   "source": [
    "Pour un réseau de neurones, calculer $l^*_y(\\bar{x}, \\bar{y})$ implique de résoudre un problème d'optimisation non convexe qui est en général intraitable. Nous étudions dans ici une relaxation convexe qui permet de calculer une borne supérieure de $l^*_y$ : $L_y(\\bar{x}, \\bar{y}) \\ge l^*_y(\\bar{x}, \\bar{y})$. De ce fait, comme discuté précédemment lorsque $L_y(\\bar{x}, \\bar{y}) < 0$, nous avons une certification de la robustesse du réseau pour l'input $(\\bar{x}, \\bar{y})$.\n",
    "\n",
    "#### Transformation du problème en un problème QCQP\n",
    "\n",
    "La source de la non-convexité du programmae d'optimisation défini dans la section précédente vient des contraintes ReLU du réseau. Considérons alors une contrainte ReLU de la forme $z = max(z,0)$. Cette contrainte peut s'exprimer de manière équivalente sous les trois contraintes suivantes : $(i) z \\ge x, (ii) z \\ge 0$ et $ (iii) z(z-x) = 0$. En effet, la contrainte $(iii)$ assure que $z$ est égale soit à 0 soit à $x$ et les deux autres contraintes $(i)$ et $(ii)$ s'assurent que $z$ est au moins plus grand qie ces deux valeurs d'égalités. Cette reformulation de la contrainte ReLU nous permet de remplacer des contraintes non linéaires du problème d'optimisation par des contraintes linéaires et quadratiques, transformant ainsi le programme en un programme quadratique à contraintes quadratiques (<a href = \"https://en.wikipedia.org/wiki/Quadratically_constrained_quadratic_program\"> QCQP </a> en anglais). Les programmes d'optimisation QCQP sont en général NP-Hard. Ce sont des programmes qui entre dans la boite des classiques programmes d'optimisation qui peuvent se résoudre à l'aide d'une relaxion convexe SDP.\n",
    "\n",
    "#### Rélaxation d'un réseau de neurones multicouches ReLU\n",
    "\n",
    "##### <u>Cas d'un réseau de neurones à une seule couche ReLU</u> :\n",
    "\n",
    "Nous montrons d'abord comment ce QCQP peut être relaxé en un programme SDP pour les réseaux à une couche cachée. La relaxation pour les couches multiples est une extension simple et est présentée à la fin de cette section.\n",
    "\n",
    "Considérons un réseau de neurones avec un seule couche cachée de $m$ unités. Soit un input $x \\in R^d$ du réseau. L'activation de la couche cachée sera notée $z \\in R^m$. Elle est reliée à $x$ par la relation $z = ReLU(Wx)$ où $W \\in R^{m \\times d}$ sont les poids du réseau.\n",
    "\n",
    "Supposons que nous notre input est borné i.e. nous avons $l, u \\in R^d$ tels que $l_j \\le x_j \\le u_j$. Par exemple, dans le cas de notre modèle d'attaqueur $L_\\infty$, nous avons $l = \\bar{x} - \\varepsilon \\mathbb{1}$ et $l = \\bar{x} =u + \\varepsilon \\mathbb{1}$ où $\\bar{x}$ est un input correcte et non pertubée.\n",
    "\n",
    "Nous nous intéressons ici à la maximisation de l'objectif : $f(x) = c^Tz$ où $c \\in R^m$, $c = c_y - c_{\\bar{y}}$.\n",
    "\n",
    "Nous utilisons la transformation des contraintes ReLU en contraintes linéaires et quadratiques, ce qui nous d'écrire un QCQP. A ce QCQP nous rajoutons la contrainte que les inputs sont bornées : $l_j \\le x_j \\le u_j$, ce qui est équivalent à une contrainte quadratique $(x_j -l_j)(x_j -u_j)≤0$, que nous réécrivons comme suit $x^2_j ≤(l_j +u_j)x_j -l_ju_j$. Nous avons alors le QCQP suivant :\n",
    "\n",
    "\n",
    "$$l^*_y(\\bar{x}, \\bar{y}) = f_{QCQP} =  max_{x,z} c^Tx^L $$\n",
    "\n",
    "<center>Sous contrainte : $z \\ge 0, z \\ge Wx, z^2 = z ⊙ (Wx)$ (contraintes du réseau)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  x^2 ≤ (l+u)⊙x - l ⊙ u $  (contraintes du modèle d'attaque)</center>\n",
    "\n",
    "<br>\n",
    "Nous relaxons maintenant ce dernier programme QCQP non convexe (3) en un SDP convexe. L'idée de base est d'introduire un nouvel ensemble de variables représentant tous les monômes linéaires et quadratiques en x et z ; les contraintes dans (3) peuvent alors être écrites comme des fonctions linéaires de ces nouvelles variables.\n",
    "\n",
    "<br>\n",
    "Par exemple, en posant $P = vv^T$ où $ v =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\[0.5mm]\n",
    "x\\\\[0.5mm]\n",
    "z\\\\\n",
    "\\end{pmatrix}\n",
    "$. Ainsi $P =  \\begin{pmatrix} \n",
    "1 & x & z \\\\ \n",
    "x & x^2 & xz \\\\ \n",
    "z & xz & z^2\n",
    "\\end{pmatrix}$ =  $\\begin{pmatrix} \n",
    "P[1] & P[x^T] & P[z^T] \\\\ \n",
    "P[x] & P[xx^T] & P[xz^T] \\\\ \n",
    "P[z] & P[zx^T] & P[zz^T]\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Ainsi, la relation convexe du programme principal peut sécrire omme suit :\n",
    "\n",
    "$$f_{QCQP} =  max_{P} c^TP[z] $$\n",
    "\n",
    "<center>Sous contrainte : $P[x] \\ge 0, P[z] \\ge WP[x], diag(P[zz^T]) = diag(WP[xz^T])$ (contraintes ReLU)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  diag(P[xx^T]) \\le (l+u)⊙P[x] - l ⊙ u$  (contraintes du modèle d'attaque)</center>\n",
    "<center> $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:  P[1] = 1, P≽0$  (contraintes du matriciels)</center>\n",
    "\n",
    "<br>\n",
    "Notons que nous considérons l'ensemble des matrices $P$ telles que $P ≽ 0$. Cet ensemble est convexe et est un sur-ensemble de l'ensemble original non-convexe. Par conséquent, le programme ci-dessus est un programme SDP ci-dessus fournissant une relaxation du QCQP avec $f_{SDP} ≥ f_{QCQP}$ , fournissant une borne supérieure sur $l^*_y(\\bar{x}, \\bar{y})$ qui pourrait servir de certificat de robustesse. \n",
    "\n",
    "##### <u> Cas d'un réseau de neurones multicouche ReLU </u> :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0105d0-bdb4-4b4f-b744-8ae92144a4a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfe605-ebbf-4415-b096-e5a5e68fc92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5367caaa-d939-4f8b-a24e-b1a86416f5e5",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6cc765-ee6f-4ac0-b743-af43211284f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
